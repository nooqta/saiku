import Agent from "@/agents/agent";
import { PredictionRequest, PredictionResponse, LLM } from "@/interfaces/llm"; // Removed FunctionCall import
import { GoogleGenerativeAI, HarmCategory, HarmBlockThreshold, GenerationConfig, Content, Part } from "@google/generative-ai";
import dotenv from "dotenv";
import prompts from "prompts";

// Define FunctionCall interface locally, similar to googleVertexAI.ts
export interface FunctionCall {
  /**
   * The arguments to call the function with, as generated by the model in JSON
   * format or object. Validate the arguments in your code before calling your function.
   */
  arguments: string | object; // Gemini SDK provides arguments as object

  /**
   * The name of the function to call.
   */
  name: string;
}

// Load environment variables from .env file
dotenv.config();

interface GeminiLLMRequest extends PredictionRequest {
  prompt: string; // Although Gemini uses structured history, we might keep this for interface consistency
  temperature?: number;
  max_tokens?: number;
  topP?: number;
  topK?: number;
  tools?: any; // Add tools property to match agent.think
}

interface GeminiLLMResponse extends PredictionResponse {
  text: string | FunctionCall[]; // Allow array of function calls
  model: string;
  otherMetadata?: any;
}

// Mapping Saiku roles to Gemini roles
const roleMap: { [key: string]: 'user' | 'model' } = {
  user: 'user',
  assistant: 'model',
  // System prompts are handled differently in Gemini (systemInstruction)
};

export class Gemini implements LLM {
  private genAI: GoogleGenerativeAI;
  private MODEL_ID: string;
  agent: Agent;
  messages: Content[] = []; // Use Gemini's Content type
  name: string;

  constructor(
    agent: Agent,
    opts: {
      apiKey?: string;
      modelId?: string;
    }
  ) {
    this.agent = agent;
    const apiKey = opts.apiKey || process.env.GEMINI_API_KEY;
    if (!apiKey) {
      throw new Error("GEMINI_API_KEY environment variable is required.");
    }
    this.MODEL_ID = opts.modelId || process.env.GEMINI_MODEL_ID || "gemini-2.5-pro-exp-03-25"; // Default to the latest experimental model
    this.name = this.MODEL_ID;
    this.genAI = new GoogleGenerativeAI(apiKey);
  }

  private formatMessages(agentMessages: any[]): Content[] {
    const history: Content[] = [];
    agentMessages.forEach(msg => {
      const role = roleMap[msg.role];
      if (role && msg.content) {
        // Ensure content is always treated as a single Part for simplicity here
        // More complex scenarios might involve multiple parts (e.g., images)
        const parts: Part[] = typeof msg.content === 'string' ? [{ text: msg.content }] : [];
         // Attempt to handle potential function call results stored as objects/strings
         if (typeof msg.content !== 'string') {
            try {
                parts.push({ text: JSON.stringify(msg.content) });
            } catch (e) {
                parts.push({ text: String(msg.content)}); // Fallback
            }
         }

        if (parts.length > 0) {
             // Check if the last message has the same role; if so, merge parts
             const lastMessage = history[history.length - 1];
             if (lastMessage && lastMessage.role === role) {
                 lastMessage.parts.push(...parts);
             } else {
                 history.push({ role, parts });
             }
        }
      }
    });
    // Gemini requires alternating user/model roles. Merge consecutive messages of the same role if necessary.
    // This basic implementation assumes simple alternation is mostly handled by Saiku's structure.
    // A more robust implementation might need explicit merging logic.
    return history;
  }

  async predict(
    request: GeminiLLMRequest
  ): Promise<GeminiLLMResponse> {
    try {
      const agentSense = await this.agent.sense();
      const systemInstruction = `
        My name is ${agentSense.agent.name}. I'm an agent that helps automate your tasks.
        You are ${JSON.stringify(agentSense.current_user)}
        The current context is: ${JSON.stringify(agentSense)}
        Instructions:
        - Analyze the user's request.
        - If the request requires information not currently known (e.g., real-time data, web content, specific file contents), you MUST use one of the available functions (tools) to retrieve it. Available tools include web searching, web scraping, file reading, etc. Do not state you cannot access external resources if a relevant tool is available.
        - If you have the information or can fulfill the request directly, provide the answer.
        - When calling a function, use the correct function name and provide the arguments in the required format based on the function's parameters schema.
      `;

      this.messages = this.formatMessages(this.agent.messages);
      const latestMessageContent = this.agent.messages[this.agent.messages.length - 1]?.content;

      if (!latestMessageContent || typeof latestMessageContent !== 'string') {
          throw new Error("Last message content is missing or not a string.");
      }

      // Extract the function declarations from the request.tools structure
      const functionDeclarations = request.tools?.map((tool: any) => tool.function).filter(Boolean);

      const model = this.genAI.getGenerativeModel({
         model: this.MODEL_ID,
         systemInstruction: { role: "system", parts: [{ text: systemInstruction }] },
         // Pass the extracted function declarations correctly
         tools: functionDeclarations && functionDeclarations.length > 0 ? [{ functionDeclarations }] : undefined,
      });

      const generationConfig: GenerationConfig = {
        temperature: request.temperature ?? 0.7, // Default temperature
        maxOutputTokens: request.max_tokens ?? 2048, // Default max tokens
        topP: request.topP ?? 0.95,
        topK: request.topK ?? 40,
      };

      const safetySettings = [
        { category: HarmCategory.HARM_CATEGORY_HARASSMENT, threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE },
        { category: HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE },
        { category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE },
        { category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE },
      ];

      // Start chat needs history without the last message, which is sent via sendMessage
      const history = this.messages.slice(0, -1);

      const chat = model.startChat({ history, generationConfig, safetySettings });

      const result = await chat.sendMessage(latestMessageContent);
      const response = result.response;

      if (!response) {
        throw new Error("No response received from the Gemini model.");
      }

      const functionCalls = response.functionCalls();
      let responseContent: string | FunctionCall[]; // Use the updated type

      if (functionCalls && functionCalls.length > 0) {
        // Priority 1: Use the dedicated functionCalls field
        responseContent = functionCalls.map(fc => ({
          name: fc.name,
          arguments: fc.args, // Gemini SDK provides arguments as object
        }));
        // console.log(`[Gemini Predict] Detected ${responseContent.length} function call(s) via functionCalls().`); // Removed log
      } else {
        // No function calls detected via functionCalls(), return the text content
        responseContent = response.text();
        // console.log("[Gemini Predict] No function calls detected via functionCalls(), returning text content."); // Removed log
      }

      return {
        text: responseContent, // Return string or FunctionCall[]
        model: this.MODEL_ID,
      };
    } catch (error: any) {
      console.error(`Gemini API error: ${error.message}`);
      // Attempt to return a structured error if possible
      return {
          text: `Error interacting with Gemini: ${error.message}`,
          model: this.MODEL_ID,
          otherMetadata: { error: true, details: error.toString() }
      }
    }
  }

  async interact(): Promise<void> {
    const decision = await this.agent.think();

    // Check if the response contains tool calls (FunctionCall[])
    const toolCalls = Array.isArray(decision.text) ? decision.text : [];
    const content = typeof decision.text === 'string' ? decision.text : null;

    // Add the raw LLM response message to history (might need adjustment based on Gemini structure)
    // Assuming decision contains the raw response structure needed for history
    // This part might need refinement if decision structure differs significantly from OpenAI
    // For now, we'll push a simplified assistant message if content exists
    if (content) {
        this.agent.messages.push({ role: "assistant", content: content });
        this.agent.displayMessage(content);
    } else if (toolCalls.length > 0) {
        // Add the assistant message that contained the tool call request (might be implicit)
        // This might need adjustment based on how Gemini structures this.
        this.agent.messages.push({ role: "assistant", content: null, tool_calls: toolCalls }); // Mimic OpenAI structure

        for (const toolCall of toolCalls) {
            let actionName = toolCall.name;
            let args = toolCall.arguments; // Arguments are already objects
            let result: any = "";

            // Avoid re-executing failed actions immediately
            if (this.agent.memory.lastAction === actionName && this.agent.memory.lastActionStatus === 'failure') {
        this.agent.updateMemory({
          lastAction: null,
          lastActionStatus: null,
        });
        this.agent.displayMessage(`Skipping failed action: ${actionName}`);
        return;
      }

      try {
        // Gemini SDK provides args as object, no need to parse JSON string
        if (actionName === 'execute_code' && !this.agent.options.allowCodeExecution) {
          const { answer } = await prompts({
            type: "confirm",
            name: "answer",
            message: `Do you want to execute the code for function ${actionName}?`,
            initial: true
          });
          if (!answer) {
            result = "Code execution cancelled by user.";
          } else {
            result = await this.agent.act(actionName, args);
          }
        } else {
          result = await this.agent.act(actionName, args);
        }
         this.agent.updateMemory({ lastAction: actionName, lastActionStatus: 'success' });
      }
      catch (error: any) {
        result = `Error executing action ${actionName}: ${error.message || error}`;
        console.error(result);
         this.agent.updateMemory({ lastAction: actionName, lastActionStatus: 'failure' });
      }

            // Add the outcome of the action as a message for context
            // Mimic OpenAI's tool role structure
            this.agent.messages.push({
                // tool_call_id: toolCall.id, // Gemini SDK doesn't seem to provide a direct equivalent ID easily
                role: "tool", // Use 'tool' role if Saiku standardizes on this
                name: actionName,
                content: typeof result === 'string' ? result : JSON.stringify(result), // Ensure content is stringified if object
            });
             this.agent.displayMessage(`Executed: ${actionName}\nResult: ${typeof result === 'string' ? result : JSON.stringify(result)}`);
        }
        // After processing all tool calls, potentially make another prediction call
        // This mimics OpenAI's flow where the results are sent back to the model.
        // This might require adjustments based on Saiku's agent loop.
        // For now, we stop after executing the tools in this turn.
        // await this.interact(); // Recursive call might be needed depending on agent design

    } else {
         this.agent.displayMessage("Received an empty or non-actionable response from the LLM.");
    }

  }
}
